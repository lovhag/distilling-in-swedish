# distilling-in-swedish
This project investigates the performance and efficiency of knowledge distillation for NER models.

It utilizes files from the following locations:
* `transformers/examples/token-classification` from the forked Transformers repo (https://github.com/lovhag/transformers), and
* NCRpp, also forked (https://github.com/lovhag/NCRFpp).

An example of how the code can be run is seen in `distilling-in-swedish/train_on_NER_old_any_model_kd_titan.sh`.
